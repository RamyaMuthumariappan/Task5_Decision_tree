# -*- coding: utf-8 -*-
"""Task5_Decision_Tree.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KdGvUv2WI8MCwwTOSCZI1B3NLMdYIbZC
"""

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Load dataset
heart_df = pd.read_csv('heart.csv')

# Feature matrix X and label y
X = heart_df.drop('target', axis=1)
y = heart_df['target']

# Split data into train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

# 1. Train a Decision Tree Classifier with default depth
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

# Visualize the tree
plt.figure(figsize=(16, 8))
plot_tree(dt, feature_names=X.columns, class_names=['No Disease', 'Disease'],
          filled=True, fontsize=8)
plt.title("Decision Tree on Heart Dataset")
plt.show()

# Evaluate accuracy
dt_pred = dt.predict(X_test)
dt_acc = accuracy_score(y_test, dt_pred)
print(f"Decision Tree Accuracy: {dt_acc:.4f}")

# 2. Analyze overfitting by controlling tree depth
depths = range(1, 16)
train_accuracies = []
test_accuracies = []

for d in depths:
    model = DecisionTreeClassifier(max_depth=d, random_state=42)
    model.fit(X_train, y_train)
    train_accuracies.append(model.score(X_train, y_train))
    test_accuracies.append(model.score(X_test, y_test))

plt.plot(depths, train_accuracies, label='Train Accuracy')
plt.plot(depths, test_accuracies, label='Test Accuracy')
plt.xlabel('Tree Depth')
plt.ylabel('Accuracy')
plt.title('Decision Tree Depth vs. Accuracy')
plt.legend()
plt.show()

# 3. Train a Random Forest and compare accuracy
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)
rf_acc = accuracy_score(y_test, rf_pred)
print(f"Random Forest Accuracy: {rf_acc:.4f}")

# Compare performance
print(f"Decision Tree Accuracy: {dt_acc:.4f}")
print(f"Random Forest Accuracy: {rf_acc:.4f}")

# 4. Interpret feature importances from Random Forest
importances = rf.feature_importances_
for feat, importance in zip(X.columns, importances):
    print(f"{feat}: {importance:.4f}")

plt.figure(figsize=(10,6))
pd.Series(importances, index=X.columns).sort_values().plot.barh()
plt.title('Feature Importances (Random Forest)')
plt.show()

# 5. Evaluate using cross-validation (for Random Forest)
cv_scores = cross_val_score(rf, X, y, cv=5)
print(f"Random Forest CV Accuracy: {cv_scores.mean():.4f} (std: {cv_scores.std():.4f})")